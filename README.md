# AI-Code-Vulnerabilities

## Project Title - Exploring Vulnerabilities in AI-Generated Code

### Collaborators  
This project was a collaborative effort by:  
- **Philipa Kolade** <!--- (https://github.com/yourusername) -->
- **Smith Adoctor** <!--- (https://github.com/teammate1) -->
- **Dorcas Owusu** <!--- (https://github.com/teammate2) -->
  
### Project Summary  
This project analyzes security vulnerabilities in code generated by five major Large Language Models (LLMs): ChatGPT, Microsoft Copilot, Google Gemini, Claude AI, and Llama. The goal is to assess the security risks associated with AI-generated programs and propose strategies to improve code safety.  

### Problem Statement  
AI-powered coding tools significantly enhance developer productivity, but they often generate code without proper security measures. This study investigates common vulnerabilities in LLM-generated code, including hardcoded secrets, weak cryptographic implementations, and SQL injection risks.  

### Method 
- Used structured prompts to generate Python code for **web applications, authentication, and cryptography**.  
- Employed **automated vulnerability scanners** (**Bandit, Semgrep**) and **manual code reviews** to identify security flaws.  
- Compared **initial** and **corrected** AI-generated outputs to assess whether LLMs can fix their own vulnerabilities.  

### Key Findings  
- **All models produced insecure code** by default, prioritizing functionality over security.  
- **Common vulnerabilities detected:**  
  - Hardcoded secrets (e.g., API keys, database credentials)  
  - Weak cryptographic practices (e.g., using AES-CBC without authentication)  
  - SQL injection risks (unsafe user input handling)  
  - Debug mode enabled in production (exposing server details)  
- **Some LLMs failed to fix their own security flaws**. 

### Full Report  
The full paper has been made available in this repository

